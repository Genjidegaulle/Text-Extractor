{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import delayed, Parallel\n",
    "from tkinter.filedialog import askopenfilename, asksaveasfilename\n",
    "from tkinter import Tk\n",
    "\n",
    "# This warning ignore is in place because the \"requests.get\" is unverified. \n",
    "# TODO: Find ways to securely make a request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for all article text\n",
    "def open_unknown_csv(file_in, delimination):\n",
    "    encode_index = 0\n",
    "    encoders = ['utf_8', 'latin1', 'utf_16',\n",
    "                'ascii', 'big5', 'big5hkscs', 'cp037', 'cp424',\n",
    "                'cp437', 'cp500', 'cp720', 'cp737', 'cp775',\n",
    "                'cp850', 'cp852', 'cp855', 'cp856', 'cp857',\n",
    "                'cp858', 'cp860', 'cp861', 'cp862', 'cp863',\n",
    "                'cp864', 'cp865', 'cp866', 'cp869', 'cp874',\n",
    "                'cp875', 'cp932', 'cp949', 'cp950', 'cp1006',\n",
    "                'cp1026', 'cp1140', 'cp1250', 'cp1251', 'cp1252',\n",
    "                'cp1253', 'cp1254', 'cp1255', 'cp1256', 'cp1257',\n",
    "                'cp1258', 'euc_jp', 'euc_jis_2004', 'euc_jisx0213', 'euc_kr',\n",
    "                'gb2312', 'gbk', 'gb18030', 'hz', 'iso2022_jp',\n",
    "                'iso2022_jp_1', 'iso2022_jp_2', 'iso2022_jp_2004', 'iso2022_jp_3', 'iso2022_jp_ext',\n",
    "                'iso2022_kr', 'latin_1', 'iso8859_2', 'iso8859_3', 'iso8859_4',\n",
    "                'iso8859_5', 'iso8859_6', 'iso8859_7', 'iso8859_8', 'iso8859_9',\n",
    "                'iso8859_10', 'iso8859_11', 'iso8859_13', 'iso8859_14', 'iso8859_15',\n",
    "                'iso8859_16', 'johab', 'koi8_r', 'koi8_u', 'mac_cyrillic',\n",
    "                'mac_greek', 'mac_iceland', 'mac_latin2', 'mac_roman', 'mac_turkish',\n",
    "                'ptcp154', 'shift_jis', 'shift_jis_2004', 'shift_jisx0213', 'utf_32',\n",
    "                'utf_32_be', 'utf_32_le', 'utf_16', 'utf_16_be', 'utf_16_le',\n",
    "                'utf_7', 'utf_8', 'utf_8_sig']\n",
    "\n",
    "    data = open_file(file_in, encoders[encode_index], delimination)\n",
    "    encoder = encoders[encode_index]\n",
    "    while type(data) == str:\n",
    "        print(\"Encoder error: \" + encoders[encode_index])\n",
    "        if encode_index < len(encoders) - 1:\n",
    "            encode_index = encode_index + 1\n",
    "            try:\n",
    "                print(\"Trying encoder: \" + encoders[encode_index])\n",
    "                data = open_file(file_in, encoders[encode_index], delimination)\n",
    "            except:\n",
    "                continue\n",
    "            print(\"\")\n",
    "            encoder = encoders[encode_index]\n",
    "        else:\n",
    "            print(\"Can't find appropriate encoder\")\n",
    "            input(\"Program Terminated. Press Enter to continue...\")\n",
    "            exit()\n",
    "\n",
    "    return data\n",
    "\n",
    "def scrape_urls_text(url):\n",
    "    text = ''\n",
    "    \n",
    "    # Handling non-URLs\n",
    "    if url == None or type(url) == float:\n",
    "        print(\"No URL provided\")\n",
    "        return 'Invalid URL or Non-existant URL'        \n",
    "\n",
    "    # Retrieving text from HTML tags\n",
    "    try:\n",
    "        # Handling incorrectly formatted URLs\n",
    "        if 'http' not in url[:5] and 'www.' in url:\n",
    "            url = 'https://' + url\n",
    "            \n",
    "        html = requests.get(url, verify=False, timeout=(5, 5))\n",
    "        \n",
    "        if html.status_code != 200:\n",
    "            if 'https' in url:\n",
    "                html = requests.get('http' + url[5:], verify=False, timeout=(5,5))\n",
    "                if html.status_code != 200:\n",
    "                    raise\n",
    "            else:\n",
    "                raise \n",
    "\n",
    "        soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        # kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup.body.get_text(separator=' ')\n",
    "        tex = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text) # removing additional characters\n",
    "        text = ' '.join([s for s in str(tex).split() if len(s)<15]) # only accept valid words\n",
    "        \n",
    "    # Catch for timeout or retry errors due to proxy\n",
    "    except:\n",
    "        print(\"Failure: \", url)\n",
    "        return 'Unable to access website'\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_text(urls):\n",
    "    # Actual web scraping!\n",
    "\n",
    "    # First extracting samsung-specific urls\n",
    "    print(\"Extracting text from all URLs...\")\n",
    "    grabbed_text = Parallel(n_jobs=-1)\\\n",
    "        (delayed(scrape_urls_text)(url)\n",
    "         for url in urls)\n",
    "    print(\"Extraction complete!\")\n",
    "    \n",
    "    return grabbed_text\n",
    "\n",
    "def open_file(file_in, encoder, delimination):\n",
    "    try:\n",
    "        data = pd.read_csv(file_in, low_memory=False, encoding=encoder, delimiter=delimination)\n",
    "        print(\"Opened file using encoder: \" + encoder)\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Encoder Error for: \" + encoder)\n",
    "        return \"Encode Error\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Program: Text Extractor\")\n",
    "print(\"Release: 0.0.1\")\n",
    "print(\"Date: 2019-12-09\")\n",
    "print(\"Author: David Liau\")\n",
    "print()\n",
    "print()\n",
    "print(\"This program extracts all text from a given list of websites using Beautiful Soup.\")\n",
    "print(\"Credit to Brian Neely (github.com/neelybd) for supplying multiple help functions.\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "# Hide Tkinter GUI\n",
    "Tk().withdraw()\n",
    "\n",
    "# Find input file\n",
    "print(\"Select File in\")\n",
    "file_in = askopenfilename(initialdir=\"../\", title=\"Select input file\",\n",
    "                          filetypes=((\"Comma Separated Values\", \"*.csv\"), (\"all files\", \"*.*\")))\n",
    "if not file_in:\n",
    "    input(\"Program Terminated. Press Enter to continue...\")\n",
    "    exit()\n",
    "\n",
    "# Set output file\n",
    "print(\"Select File out\")\n",
    "file_out = asksaveasfilename(initialdir=file_in, title=\"Select output file\",\n",
    "                             filetypes=((\"Comma Separated Values\", \"*.csv\"), (\"all files\", \"*.*\")))\n",
    "if '.' not in file_out[-4:]:\n",
    "    file_out = file_out + '.csv'\n",
    "\n",
    "if not file_out:\n",
    "    input(\"Program Terminated. Press Enter to continue...\")\n",
    "    exit()\n",
    "\n",
    "# Identify CSV Encoding\n",
    "data = open_unknown_csv(file_in, \",\")\n",
    "    \n",
    "data['Text'] = extract_text(data.iloc[:,0])\n",
    "\n",
    "# output = pd.DataFrame(text, columns=['Text'])\n",
    "output = data\n",
    "\n",
    "# Write CSV\n",
    "print(\"Writing CSV File...\")\n",
    "output.to_csv(file_out, index=False)\n",
    "print(\"Wrote CSV File!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
